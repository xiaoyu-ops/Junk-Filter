"""
FastAPI HTTP Server for Python Backend
ç”¨äºæä¾›è¯„ä¼°å’ŒèŠå¤©æ¥å£ç»™å‰ç«¯å’Œ Go åç«¯
"""

import asyncio
import json
import logging
import os
from typing import Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from config import settings
from main import Database, Redis
from agents.content_evaluator import ContentEvaluationAgent

# LLM Integration
try:
    from langchain_openai import ChatOpenAI
    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False
    logger_temp = logging.getLogger(__name__)
    logger_temp.warning("LangChain/OpenAI not available, will use fallback responses")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=settings.log_level)
logger = logging.getLogger(__name__)

# ======================== æ•°æ®æ¨¡å‹ ========================

class EvaluationRequest(BaseModel):
    """è¯„ä¼°è¯·æ±‚"""
    title: str
    content: str
    temperature: Optional[float] = None
    topP: Optional[float] = None
    maxTokens: Optional[int] = None
    max_tokens: Optional[int] = None  # å‘åå…¼å®¹


class EvaluationResponse(BaseModel):
    """è¯„ä¼°ç»“æœ"""
    title: str
    content: str
    innovation_score: int
    depth_score: int
    decision: str  # INTERESTING, BOOKMARK, SKIP
    tldr: str
    key_concepts: list


class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    task_id: str
    message: str
    temperature: Optional[float] = None


class TaskChatRequest(BaseModel):
    """ä»»åŠ¡ç‰¹å®šçš„èŠå¤©è¯·æ±‚ï¼ˆAgent è°ƒä¼˜ä¸å’¨è¯¢ï¼‰"""
    message: str
    task_id: int
    agent_context: dict  # {task_metadata, chat_history, recent_cards, current_config, ...}


class TaskChatResponse(BaseModel):
    """ä»»åŠ¡èŠå¤©çš„è‡ªç„¶è¯­è¨€å›å¤"""
    reply: str
    referenced_card_ids: list = []
    parameter_updates: Optional[dict] = None
    context_used: dict = {}


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str
    database: str
    redis: str
    llm: str


# ======================== åº”ç”¨åˆå§‹åŒ– ========================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨
    try:
        await Database.initialize()
        await Redis.initialize()
        logger.info("âœ“ Python API Server initialized")
    except Exception as e:
        logger.error(f"âœ— Initialization failed: {e}")
        raise

    yield

    # å…³é—­
    await Database.close()
    await Redis.close()
    logger.info("âœ“ Python API Server shutdown")


# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="Junk Filter Python Backend API",
    description="è¯„ä¼°å’ŒèŠå¤©æ¥å£",
    version="1.0.0",
    lifespan=lifespan
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰æ¥æºï¼ˆå¼€å‘ç¯å¢ƒï¼‰
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–è¯„ä¼° Agent
model_id = os.getenv("LLM_MODEL_ID") or os.getenv("llm_model_id") or settings.llm_model_id or settings.llm_model or "gpt-4o"
api_key = os.getenv("OPENAI_API_KEY") or os.getenv("openai_api_key") or settings.openai_api_key or os.getenv("OPENAI_API_KEY", "")
api_base = os.getenv("LLM_BASE_URL") or os.getenv("llm_base_url") or settings.llm_base_url or os.getenv("LLM_BASE_URL", "https://api.openai.com/v1")

logger.info(f"LLM Configuration: Model={model_id}, Base={api_base}")

evaluator = ContentEvaluationAgent(
    model=model_id,
    api_key=api_key,
    api_base=api_base
)


# ======================== å¥åº·æ£€æŸ¥ ========================

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    db_status = "connected" if Database.get_pool() else "disconnected"
    redis_status = "connected" if Redis.get_client() else "disconnected"
    llm_status = "configured" if api_key else "unconfigured"

    return HealthResponse(
        status="healthy",
        database=db_status,
        redis=redis_status,
        llm=llm_status
    )


# ======================== è¯„ä¼°æ¥å£ ========================

@app.post("/api/evaluate", response_model=EvaluationResponse)
async def evaluate(request: EvaluationRequest):
    """
    è¯„ä¼°å†…å®¹ï¼ˆåŒæ­¥è°ƒç”¨ï¼‰

    Args:
        request: åŒ…å« title å’Œ content

    Returns:
        EvaluationResponse: è¯„ä¼°ç»“æœ

    Raises:
        HTTPException: è¯„ä¼°å¤±è´¥æ—¶è¿”å› 500
    """
    try:
        logger.info(f"Evaluating content: {request.title[:50]}")

        # è°ƒç”¨è¯„ä¼° Agent
        result = await evaluator.evaluate(
            title=request.title,
            content=request.content,
            url="",
            temperature=request.temperature or settings.llm_temperature,
            max_tokens=request.max_tokens or settings.llm_max_tokens,
        )

        logger.info(f"âœ“ Evaluation completed: {result.get('decision')}")

        return EvaluationResponse(
            title=request.title,
            content=request.content,
            innovation_score=result.get("innovation_score", 5),
            depth_score=result.get("depth_score", 5),
            decision=result.get("decision", "BOOKMARK"),
            tldr=result.get("tldr", ""),
            key_concepts=result.get("key_concepts", [])
        )

    except Exception as e:
        logger.error(f"âœ— Evaluation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/evaluate/stream")
async def evaluate_stream(request: EvaluationRequest):
    """
    æµå¼è¯„ä¼°ï¼ˆç”¨äºå‰ç«¯ SSEï¼‰

    æµå¼è¿”å›è¯„ä¼°è¿‡ç¨‹ä¸­çš„æ­¥éª¤å’Œæœ€ç»ˆç»“æœ
    """
    async def stream_generator():
        try:
            logger.info(f"Starting streaming evaluation: {request.title[:50]}")

            # å‘é€å¼€å§‹äº‹ä»¶
            yield "data: " + json.dumps({"status": "processing", "phase": "starting"}) + "\n\n"
            await asyncio.sleep(0.1)

            # å‘é€è¯„ä¼°ä¸­äº‹ä»¶
            yield "data: " + json.dumps({"status": "processing", "phase": "evaluating"}) + "\n\n"

            # è°ƒç”¨è¯„ä¼° Agent
            result = await evaluator.evaluate(
                title=request.title,
                content=request.content,
                url="",
                temperature=request.temperature or settings.llm_temperature,
                max_tokens=request.maxTokens or request.max_tokens or settings.llm_max_tokens,
            )

            # å‘é€å®Œæˆäº‹ä»¶å’Œç»“æœ
            yield "data: " + json.dumps({
                "status": "completed",
                "result": {
                    "title": request.title,
                    "content": request.content,
                    "innovation_score": result.get("innovation_score", 5),
                    "depth_score": result.get("depth_score", 5),
                    "decision": result.get("decision", "BOOKMARK"),
                    "tldr": result.get("tldr", ""),
                    "key_concepts": result.get("key_concepts", [])
                }
            }) + "\n\n"

            logger.info(f"âœ“ Streaming evaluation completed")

        except Exception as e:
            logger.error(f"âœ— Streaming evaluation failed: {e}")
            yield "data: " + json.dumps({
                "status": "error",
                "error": str(e)
            }) + "\n\n"

    return StreamingResponse(
        stream_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx ç¼“å†²
        }
    )


# ======================== ä»»åŠ¡ç‰¹å®šèŠå¤©æ¥å£ï¼ˆAgent è°ƒä¼˜ï¼‰ ========================

@app.post("/api/task/{task_id}/chat")
async def task_chat(task_id: int, request: TaskChatRequest):
    """
    ä»»åŠ¡ç‰¹å®šçš„èŠå¤©ç«¯ç‚¹ - Agent è°ƒä¼˜ä¸å’¨è¯¢

    ç”¨æˆ·å¯ä»¥ï¼š
    1. æŸ¥è¯¢ä»»åŠ¡æ‰§è¡Œè¿›åº¦
    2. è°ƒæ•´ Agent çš„è¿‡æ»¤è§„åˆ™å’Œå‚æ•°
    3. è§£é‡Šç‰¹å®šè¯„ä¼°å¡ç‰‡çš„å†³ç­–
    4. è·å–åŸºäºä¸Šä¸‹æ–‡çš„å»ºè®®

    Args:
        task_id: ä»»åŠ¡ ID
        request: åŒ…å«ç”¨æˆ·æ¶ˆæ¯å’Œå®Œæ•´ä¸Šä¸‹æ–‡çš„è¯·æ±‚

    Returns:
        SSE Stream æ ¼å¼çš„è‡ªç„¶è¯­è¨€å›å¤
    """
    async def stream_generator():
        try:
            logger.info(f"[Task Chat] Task {task_id}: {request.message[:50]}")

            # å‘é€åˆå§‹åŒ–äº‹ä»¶
            yield "data: " + json.dumps({"status": "processing", "phase": "analyzing"}) + "\n\n"
            await asyncio.sleep(0.1)

            # ==================== æ„å»º Agent æç¤ºè¯ ====================
            task_meta = request.agent_context.get("task_metadata", {})
            chat_history = request.agent_context.get("chat_history", [])
            recent_cards = request.agent_context.get("recent_cards", [])
            current_config = request.agent_context.get("current_config", {})

            # æ ¼å¼åŒ–èŠå¤©å†å²
            chat_history_str = ""
            if chat_history:
                for msg in chat_history[-5:]:  # æœ€å¤šæ˜¾ç¤ºæœ€è¿‘ 5 æ¡
                    role = "ç”¨æˆ·" if msg.get("role") == "user" else "AI"
                    content = msg.get("content", "")[:100]  # æˆªæ–­é•¿æ¶ˆæ¯
                    chat_history_str += f"{role}: {content}\n"

            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = f"""ä½ æ˜¯ Junk Filter çš„ Agent è°ƒä¼˜åŠ©æ‰‹ã€‚ä½ çš„è§’è‰²æ˜¯å¸®åŠ©ç”¨æˆ·ç†è§£å’Œä¼˜åŒ– RSS å†…å®¹è¯„ä¼°çš„ Agentã€‚

ã€å½“å‰ä»»åŠ¡ã€‘
åç§°: {task_meta.get('name', 'Unknown')}
æè¿°: {request.agent_context.get('task_description', 'N/A')}
ä¼˜å…ˆçº§: {task_meta.get('priority', 'N/A')}

ã€å½“å‰é…ç½®ã€‘
- Temperature: {current_config.get('temperature', 0.7)}
- Top P: {current_config.get('topP', 0.9)}
- Max Tokens: {current_config.get('maxTokens', 2000)}
- è¿‡æ»¤è§„åˆ™: {current_config.get('filter_rules', 'default')}

ã€æœ€è¿‘å¯¹è¯ã€‘
{chat_history_str if chat_history_str else 'ï¼ˆæ— å†å²å¯¹è¯ï¼‰'}

ã€æœ€è¿‘çš„è¯„ä¼°å¡ç‰‡æ‘˜è¦ã€‘
{len(recent_cards)} å¼ å¡ç‰‡å·²è¯„ä¼°

ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·ï¼š
1. æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ï¼š"ç°åœ¨çš„æ‰§è¡Œè¿›åº¦å¦‚ä½•ï¼Ÿ" â†’ æä¾›å¤„ç†æ•°é‡ã€æˆåŠŸç‡ã€æœ€è¿‘è¯„ä¼°çš„å†…å®¹ç±»å‹
2. è°ƒæ•´è¿‡æ»¤è§„åˆ™ï¼š"æ¥ä¸‹æ¥å¤šå…³æ³¨æ·±åº¦å­¦ä¹ çš„è®ºæ–‡" â†’ å»ºè®®å¦‚ä½•ä¿®æ”¹ filter_rules
3. è§£é‡Šè¯„ä¼°å†³ç­–ï¼š"ä¸ºä»€ä¹ˆè¿™å¼ å¡ç‰‡è¢«æ ‡è®°ä¸º SKIPï¼Ÿ" â†’ æŸ¥é˜…å¡ç‰‡æ•°æ®ç»™å‡ºè§£é‡Š
4. æ€§èƒ½å»ºè®®ï¼š"å¦‚ä½•æ”¹è¿›è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Ÿ" â†’ å»ºè®®è°ƒæ•´å‚æ•°æˆ–è§„åˆ™

å›å¤æ—¶ï¼š
- ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¸­æ–‡
- ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜
- å¦‚æœæ¶‰åŠå‚æ•°ä¿®æ”¹ï¼Œåœ¨å›å¤ä¸­æ¸…æ¥šåœ°è¯´æ˜"å»ºè®®: å°† temperature æ”¹ä¸º X"
- å¦‚æœéœ€è¦å¼•ç”¨å¡ç‰‡ï¼Œè¯·è¯´"å‚è€ƒå¡ç‰‡ #123"æ ¼å¼"""

            # å‘é€åˆ†æä¸­äº‹ä»¶
            yield "data: " + json.dumps({"status": "processing", "phase": "generating"}) + "\n\n"
            await asyncio.sleep(0.2)

            # ==================== è°ƒç”¨ LLM ====================
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šç›´æ¥ç”Ÿæˆå›å¤è€Œä¸èµ°è¯„ä¼° Agent
            # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥æœ‰ç‹¬ç«‹çš„ Chat Agent æˆ–è°ƒç”¨ LLM API

            reply = await generate_task_chat_reply(
                user_message=request.message,
                system_prompt=system_prompt,
                task_metadata=task_meta
            )

            # ==================== è§£æå›å¤ä¸­çš„å‚æ•°æ›´æ–° ====================
            parameter_updates = extract_parameter_updates(reply)
            referenced_cards = extract_referenced_cards(reply)

            # ==================== å‘é€å®Œæˆäº‹ä»¶ ====================
            yield "data: " + json.dumps({
                "status": "completed",
                "result": {
                    "reply": reply,
                    "referenced_card_ids": referenced_cards,
                    "parameter_updates": parameter_updates,
                    "context_used": {
                        "task_id": task_id,
                        "message_length": len(request.message),
                        "context_keys": list(request.agent_context.keys())
                    }
                }
            }) + "\n\n"

            logger.info(f"[Task Chat] Completed for task {task_id}")

        except Exception as e:
            logger.error(f"[Task Chat] Error: {e}", exc_info=True)
            yield "data: " + json.dumps({
                "status": "error",
                "error": str(e)
            }) + "\n\n"

    return StreamingResponse(
        stream_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


# ======================== ä»»åŠ¡èŠå¤©çš„è¾…åŠ©å‡½æ•° ========================

async def generate_task_chat_reply(user_message: str, system_prompt: str, task_metadata: dict) -> str:
    """
    ç”Ÿæˆä»»åŠ¡ç‰¹å®šçš„èŠå¤©å›å¤

    ä½¿ç”¨çœŸå®çš„ LLMï¼ˆå¦‚ OpenAIï¼‰ç”Ÿæˆè‡ªç„¶è¯­è¨€å›å¤ã€‚
    å¦‚æœ LLM ä¸å¯ç”¨ï¼Œå›é€€åˆ°è§„åˆ™åŒ¹é…ã€‚
    """
    # é¦–å…ˆå°è¯•ä½¿ç”¨çœŸå® LLM
    if LLM_AVAILABLE and settings.openai_api_key and settings.openai_api_key != "sk-xxxxx":
        try:
            return await _call_llm(user_message, system_prompt)
        except Exception as e:
            logger.warning(f"LLM call failed, falling back to rule-based responses: {e}")

    # å›é€€åˆ°è§„åˆ™åŒ¹é…
    return _fallback_rule_based_reply(user_message, task_metadata)


async def _call_llm(user_message: str, system_prompt: str) -> str:
    """ä½¿ç”¨ LangChain + OpenAI è°ƒç”¨çœŸå® LLM"""
    try:
        # å¼ºåˆ¶ä» .env è¯»å–æ¨¡å‹åï¼Œä¸ä½¿ç”¨ä»»ä½•ç¡¬ç¼–ç å€¼
        model_from_env = os.getenv("LLM_MODEL_ID") or os.getenv("llm_model_id")
        api_key_from_env = os.getenv("OPENAI_API_KEY") or os.getenv("openai_api_key")
        base_url_from_env = os.getenv("LLM_BASE_URL") or os.getenv("llm_base_url")

        logger.info(f"[LLM Call] Model from .env: {model_from_env}")
        logger.info(f"[LLM Call] Base URL from .env: {base_url_from_env}")

        # æ„å»º ChatOpenAI å‚æ•°
        llm_kwargs = {
            "api_key": api_key_from_env or settings.openai_api_key,
            "model_name": model_from_env or settings.llm_model_id,
            "temperature": settings.llm_temperature,
            "max_tokens": settings.llm_max_tokens,
            "timeout": settings.llm_timeout,
        }

        # å¦‚æœé…ç½®äº†è‡ªå®šä¹‰ base_urlï¼ˆå¦‚ä¸­è½¬ç«™ï¼‰ï¼Œä½¿ç”¨ base_url å‚æ•°
        if base_url_from_env or settings.llm_base_url:
            llm_kwargs["base_url"] = base_url_from_env or settings.llm_base_url
            logger.info(f"Using custom LLM base_url: {llm_kwargs['base_url']}")
            logger.info(f"Using model: {llm_kwargs['model_name']}")

        llm = ChatOpenAI(**llm_kwargs)

        # è°ƒç”¨ LLM
        response = await asyncio.to_thread(
            llm.invoke,
            [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]
        )

        return response.content if hasattr(response, 'content') else str(response)
    except Exception as e:
        logger.error(f"LLM API call failed: {e}")
        raise


def _fallback_rule_based_reply(user_message: str, task_metadata: dict) -> str:
    """è§„åˆ™åŒ¹é…çš„å›é€€å®ç°"""
    lower_msg = user_message.lower()

    if any(keyword in lower_msg for keyword in ["è¿›åº¦", "å¤šå°‘", "å¤„ç†", "çŠ¶æ€"]):
        return f"""å½“å‰ä»»åŠ¡ {task_metadata.get('name', 'Unknown')} çš„ç»Ÿè®¡ä¿¡æ¯ï¼š

âœ… å·²å¤„ç†æ¶ˆæ¯: 127 æ¡
â­ é«˜ä»·å€¼å†…å®¹: 12 æ¡ (9.4%)
ğŸ“Œ å·²ä¹¦ç­¾: 38 æ¡ (29.9%)
â­ï¸ è·³è¿‡: 77 æ¡ (60.6%)

æœ€è¿‘ 1 å°æ—¶å†…ï¼Œç³»ç»Ÿè¯†åˆ«å‡º 3 æ¡åˆ›æ–°åº¦å’Œæ·±åº¦éƒ½è¾¾åˆ° 8 åˆ†ä»¥ä¸Šçš„é«˜è´¨é‡å†…å®¹ã€‚

å»ºè®®: ä½ å¯ä»¥æŸ¥çœ‹æ—¶é—´è½´ä¸Šçš„é«˜è¯„åˆ†å¡ç‰‡ï¼Œäº†è§£æœ€è¿‘çš„çƒ­ç‚¹è¯é¢˜ã€‚"""

    elif any(keyword in lower_msg for keyword in ["è°ƒæ•´", "ä¿®æ”¹", "è§„åˆ™", "è¿‡æ»¤"]):
        return f"""å¥½çš„ï¼Œæˆ‘ç†è§£ä½ æƒ³è°ƒæ•´è¯„ä¼°è§„åˆ™ã€‚

å½“å‰çš„è¿‡æ»¤è§„åˆ™æ˜¯: `default`

æˆ‘å¯ä»¥å¸®åŠ©ä½ ä¿®æ”¹ä»¥ä¸‹å‚æ•°ï¼š
- **è¿‡æ»¤å…³é”®è¯**: æ¯”å¦‚æ·»åŠ  "æ·±åº¦å­¦ä¹ " æ¥ä¸“æ³¨ AI è¯é¢˜
- **è¯„ä¼°æ•æ„Ÿåº¦**: è°ƒæ•´ temperature (0.0-1.0)ï¼Œæ•°å€¼è¶Šé«˜è¶Š"æœ‰åˆ›æ„"
- **å†…å®¹é•¿åº¦åå¥½**: é€šè¿‡ max_tokens è°ƒæ•´

å»ºè®®: å‘Šè¯‰æˆ‘ä½ æƒ³å…³æ³¨çš„å…·ä½“è¯é¢˜ï¼Œæˆ‘ä¼šç»™å‡ºå‚æ•°å»ºè®®ã€‚"""

    else:
        return f"""æˆ‘å·²æ”¶åˆ°ä½ çš„é—®é¢˜ï¼š"{user_message}"

å…³äºä»»åŠ¡ {task_metadata.get('name', 'Unknown')}ï¼Œæˆ‘å¯ä»¥å¸®åŠ©ä½ ï¼š

1. **æŸ¥è¯¢è¿›åº¦**: é—®æˆ‘"ç°åœ¨çš„æ‰§è¡Œè¿›åº¦å¦‚ä½•ï¼Ÿ"è·å–æœ€æ–°ç»Ÿè®¡
2. **è°ƒæ•´è§„åˆ™**: å‘Šè¯‰æˆ‘ä½ æƒ³å…³æ³¨ä»€ä¹ˆå†…å®¹ç±»å‹
3. **è§£é‡Šå†³ç­–**: é—®æˆ‘"ä¸ºä»€ä¹ˆæŸå¼ å¡ç‰‡è¢«æ ‡è®°ä¸º SKIPï¼Ÿ"
4. **ä¼˜åŒ–å»ºè®®**: é—®æˆ‘"å¦‚ä½•æ”¹è¿›è¯„ä¼°å‡†ç¡®æ€§ï¼Ÿ"

è¯·å‘Šè¯‰æˆ‘ä½ éœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚"""


def extract_parameter_updates(reply: str) -> dict:
    """ä» AI å›å¤ä¸­æå–å‚æ•°æ›´æ–°å»ºè®®"""
    updates = {}
    # ç®€å•çš„æ­£åˆ™åŒ¹é…ç¤ºä¾‹
    if "temperature" in reply.lower():
        # æå–å»ºè®®çš„ temperature å€¼
        pass
    return updates


def extract_referenced_cards(reply: str) -> list:
    """ä» AI å›å¤ä¸­æå–å¼•ç”¨çš„å¡ç‰‡ ID"""
    import re
    card_ids = []
    # åŒ¹é… "å¡ç‰‡ #123" æ ¼å¼
    matches = re.findall(r'å¡ç‰‡\s*#?(\d+)', reply)
    card_ids = [int(m) for m in matches]
    return card_ids




@app.get("/api/chat/stream")
async def chat_stream(
    task_id: str = Query(..., description="ä»»åŠ¡ ID"),
    message: str = Query(..., description="ç”¨æˆ·æ¶ˆæ¯")
):
    """
    èŠå¤©æµå¼æ¥å£ï¼ˆç”± Go åç«¯ /api/chat/stream è°ƒç”¨ï¼‰

    ç”¨äº Go åç«¯å‘ Python è¯·æ±‚æµå¼è¯„ä¼°ï¼Œç„¶åè½¬å‘ç»™å‰ç«¯
    """
    async def stream_generator():
        try:
            logger.info(f"Chat stream for task {task_id}: {message[:50]}")

            # è¿™é‡Œå¯ä»¥æ ¹æ® task_id è·å–ä¸Šä¸‹æ–‡
            # æš‚æ—¶ç®€å•å¤„ç†ï¼šç›´æ¥è¯„ä¼°æ¶ˆæ¯å†…å®¹

            yield "data: " + json.dumps({"status": "thinking"}) + "\n\n"
            await asyncio.sleep(0.2)

            # è°ƒç”¨è¯„ä¼° Agent
            result = await evaluator.evaluate(
                title="User Query",
                content=message,
                url="",
                temperature=settings.llm_temperature,
                max_tokens=settings.llm_max_tokens,
            )

            # è¿”å›è¯„ä¼°ç»“æœ
            yield "data: " + json.dumps({
                "status": "completed",
                "text": result.get("tldr", ""),
                "decision": result.get("decision", "BOOKMARK")
            }) + "\n\n"

        except Exception as e:
            logger.error(f"âœ— Chat stream failed: {e}")
            yield "data: " + json.dumps({
                "status": "error",
                "error": str(e)
            }) + "\n\n"

    return StreamingResponse(
        stream_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


# ======================== æ ¹è·¯å¾„ ========================

@app.get("/")
async def root():
    """API æ ¹è·¯å¾„"""
    return {
        "service": "Junk Filter Python Backend API",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health"
    }


# ======================== å¯åŠ¨è„šæœ¬ ========================

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        app,
        host=settings.api_host,
        port=settings.api_port,
        workers=settings.api_workers,
        log_level=settings.log_level.lower()
    )
